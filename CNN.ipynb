{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECOQCODE OCR CNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation (Synthetic Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4b3a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "bg_dir = './real_backgrounds'\n",
    "out_dir = './eco_dataset/images'\n",
    "label_file = './eco_dataset/labels.txt'\n",
    "font_path = 'arial.ttf'  # Change as per your system (e.g., 'NotoSansCJK-Regular.ttc')\n",
    "text = 'ECOQCODE'\n",
    "num_samples = 1000  # Number of images to generate\n",
    "image_size = (224, 224)  # Output image size\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# List of background image files\n",
    "bg_files = [str(p) for p in Path(bg_dir).glob('*') if p.suffix.lower() in ['.jpg', '.png', '.jpeg']]\n",
    "\n",
    "# Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(font_path, size=24)\n",
    "except:\n",
    "    raise RuntimeError(f\"Font file not found: {font_path}\")\n",
    "\n",
    "# Save labels\n",
    "with open(label_file, 'w', encoding='utf-8') as lf:\n",
    "    for i in range(num_samples):\n",
    "        bg_path = random.choice(bg_files)\n",
    "        img = Image.open(bg_path).convert(\"RGB\").resize(image_size)\n",
    "\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Set label for binary classification\n",
    "        has_text = random.random() < 0.5  # 50% chance to insert ECOQCODE\n",
    "        label = 1 if has_text else 0\n",
    "\n",
    "        if has_text:\n",
    "            # Determine text size\n",
    "            for _ in range(10):  # Retry 10 times if text is larger than image\n",
    "                text_size = random.randint(6, 8)\n",
    "                font = ImageFont.truetype(font_path, text_size)\n",
    "                text_width = int(text_size * 0.6 * len(text))  # Approximate width calculation\n",
    "                text_height = text_size\n",
    "\n",
    "                if text_width < image_size[0] and text_height < image_size[1]:\n",
    "                    break\n",
    "            else:\n",
    "                # Skip if text cannot be inserted\n",
    "                continue\n",
    "\n",
    "            x = random.randint(0, image_size[0] - text_width)\n",
    "            y = random.randint(0, image_size[1] - text_height)\n",
    "\n",
    "            draw.text((x, y), text, font=font, fill=(0, 0, 0))\n",
    "\n",
    "        # Save file\n",
    "        filename = f\"img_{i:05d}.jpg\"\n",
    "        save_path = os.path.join(out_dir, filename)\n",
    "        img.save(save_path)\n",
    "\n",
    "        # Record label\n",
    "        lf.write(f\"{save_path}\\t{label}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc0351",
   "metadata": {},
   "source": [
    "## 2. Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79ff46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class ECOQDataset(Dataset):\n",
    "    def __init__(self, label_path, transform=None):\n",
    "        self.samples = []\n",
    "        with open(label_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                path, label = line.strip().split(\"\\t\")\n",
    "                self.samples.append((path, int(label)))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e5f2f",
   "metadata": {},
   "source": [
    "## 3. Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15126ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)  # RGB Normalization\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78e61b",
   "metadata": {},
   "source": [
    "## 4. Model Architecture (CNN Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f7489e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def get_transfer_model():\n",
    "    # Load a pre-trained ResNet18 model\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Freeze all the parameters in the feature extraction part\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final fully connected layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 1) # Output is 1 for binary classification\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1249c3",
   "metadata": {},
   "source": [
    "## 5. Data Splitting (Train/Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0978e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_file = \"./eco_dataset/labels.txt\" \n",
    "\n",
    "# Load label list and split 8:2\n",
    "with open(label_file, encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "train_lines, val_lines = train_test_split(lines, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Split files\n",
    "with open(\"eco_dataset/train_labels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_lines)\n",
    "\n",
    "with open(\"eco_dataset/val_labels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(val_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef7e26",
   "metadata": {},
   "source": [
    "## 6. Dataset and DataLoader Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def18c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# ImageNet statistics for normalization\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define image transformations for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ECOQDataset(Dataset):\n",
    "    def __init__(self, label_file, transform=None):\n",
    "        with open(label_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        self.samples = [line.strip().split('\\t') for line in lines]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor([int(label)], dtype=torch.float32)\n",
    "        return image, label\n",
    "\n",
    "# Set data paths\n",
    "train_label_path = './eco_dataset/train_labels.txt'\n",
    "val_label_path = './eco_dataset/val_labels.txt'\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = ECOQDataset(train_label_path, transform=train_transform)\n",
    "val_dataset = ECOQDataset(val_label_path, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53cab35",
   "metadata": {},
   "source": [
    "## 7. Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb1440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 25/25 [00:02<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6962\n",
      "Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 25/25 [00:02<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.6732\n",
      "Validation Accuracy: 64.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 25/25 [00:02<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.6390\n",
      "Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 25/25 [00:02<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.6506\n",
      "Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 25/25 [00:02<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.6228\n",
      "Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 25/25 [00:02<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.6112\n",
      "Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 25/25 [00:02<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.6042\n",
      "Validation Accuracy: 51.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 25/25 [00:02<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.5875\n",
      "Validation Accuracy: 53.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 25/25 [00:02<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.6005\n",
      "Validation Accuracy: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 25/25 [00:02<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.5988\n",
      "Validation Accuracy: 50.50%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the transfer learning model\n",
    "model = get_transfer_model().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, 11): # Train for 10 epochs\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        imgs, labels = imgs.to(device), labels.float().to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        preds = model(imgs)\n",
    "        # FIX: Ensure labels have the same shape as preds [batch_size, 1]\n",
    "        loss = criterion(preds, labels)\n",
    "        \n",
    "        # Backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            # Squeeze labels for accuracy calculation, as preds will be squeezed\n",
    "            labels = labels.to(device).float().squeeze()\n",
    "            outputs = model(imgs)\n",
    "            preds = (torch.sigmoid(outputs).squeeze() > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Validation Accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation and ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d88ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8534    1.0000    0.9209        99\n",
      "         1.0     1.0000    0.8317    0.9081       101\n",
      "\n",
      "    accuracy                         0.9150       200\n",
      "   macro avg     0.9267    0.9158    0.9145       200\n",
      "weighted avg     0.9275    0.9150    0.9145       200\n",
      "\n",
      "Model successfully exported to ecoq_classifier.onnx\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device).float().squeeze()\n",
    "        outputs = model(imgs)\n",
    "        preds = (torch.sigmoid(outputs).squeeze() > 0.5).float()\n",
    "\n",
    "        if preds.dim() != labels.dim():\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# Export the trained model to ONNX format\n",
    "output_onnx_path = \"ecoq_classifier.onnx\"\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device) # Example input: batch_size=1, channels=3, height=224, width=224\n",
    "\n",
    "torch.onnx.export(model,                   # trained model\n",
    "                   dummy_input,             # example input for tracing\n",
    "                   output_onnx_path,        # where to save the ONNX model\n",
    "                   export_params=True,      # store the trained parameter weights inside the model file\n",
    "                   opset_version=11,        # the ONNX version to export the model to\n",
    "                   do_constant_folding=True, # whether to execute constant folding for optimization\n",
    "                   input_names = ['input'],   # the model's input names\n",
    "                   output_names = ['output'], # the model's output names\n",
    "                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                 'output' : {0 : 'batch_size'}})\n",
    "\n",
    "print(f\"Model successfully exported to {output_onnx_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
